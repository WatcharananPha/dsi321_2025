{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b21619b-7f31-4a6d-af8c-92e3fc9ec035",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.11/site-packages (2.2.3)\n",
      "Requirement already satisfied: selenium in /opt/conda/lib/python3.11/site-packages (4.32.0)\n",
      "Requirement already satisfied: webdriver-manager in /opt/conda/lib/python3.11/site-packages (4.0.2)\n",
      "Requirement already satisfied: pyarrow in /opt/conda/lib/python3.11/site-packages (20.0.0)\n",
      "Requirement already satisfied: s3fs in /opt/conda/lib/python3.11/site-packages (2025.3.2)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.2.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.11/site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.11/site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.26 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (2.0.7)\n",
      "Requirement already satisfied: trio~=0.17 in /opt/conda/lib/python3.11/site-packages (from selenium) (0.30.0)\n",
      "Requirement already satisfied: trio-websocket~=0.9 in /opt/conda/lib/python3.11/site-packages (from selenium) (0.12.2)\n",
      "Requirement already satisfied: certifi>=2021.10.8 in /opt/conda/lib/python3.11/site-packages (from selenium) (2023.7.22)\n",
      "Requirement already satisfied: typing_extensions~=4.9 in /opt/conda/lib/python3.11/site-packages (from selenium) (4.13.2)\n",
      "Requirement already satisfied: websocket-client~=1.8 in /opt/conda/lib/python3.11/site-packages (from selenium) (1.8.0)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.11/site-packages (from webdriver-manager) (2.31.0)\n",
      "Requirement already satisfied: python-dotenv in /opt/conda/lib/python3.11/site-packages (from webdriver-manager) (1.1.0)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.11/site-packages (from webdriver-manager) (23.2)\n",
      "Requirement already satisfied: aiobotocore<3.0.0,>=2.5.4 in /opt/conda/lib/python3.11/site-packages (from s3fs) (2.22.0)\n",
      "Requirement already satisfied: fsspec==2025.3.2.* in /opt/conda/lib/python3.11/site-packages (from s3fs) (2025.3.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /opt/conda/lib/python3.11/site-packages (from s3fs) (3.11.18)\n",
      "Requirement already satisfied: aioitertools<1.0.0,>=0.5.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (0.12.0)\n",
      "Requirement already satisfied: botocore<1.37.4,>=1.37.2 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.37.3)\n",
      "Requirement already satisfied: jmespath<2.0.0,>=0.7.1 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.0.1)\n",
      "Requirement already satisfied: multidict<7.0.0,>=6.0.0 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (6.4.3)\n",
      "Requirement already satisfied: wrapt<2.0.0,>=1.10.10 in /opt/conda/lib/python3.11/site-packages (from aiobotocore<3.0.0,>=2.5.4->s3fs) (1.17.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.6.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /opt/conda/lib/python3.11/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->s3fs) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.11/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: sortedcontainers in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (2.4.0)\n",
      "Requirement already satisfied: idna in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (3.4)\n",
      "Requirement already satisfied: outcome in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0.post0)\n",
      "Requirement already satisfied: sniffio>=1.3.0 in /opt/conda/lib/python3.11/site-packages (from trio~=0.17->selenium) (1.3.0)\n",
      "Requirement already satisfied: wsproto>=0.14 in /opt/conda/lib/python3.11/site-packages (from trio-websocket~=0.9->selenium) (1.2.0)\n",
      "Requirement already satisfied: pysocks!=1.5.7,<2.0,>=1.5.6 in /opt/conda/lib/python3.11/site-packages (from urllib3[socks]<3,>=1.26->selenium) (1.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.11/site-packages (from requests->webdriver-manager) (3.3.0)\n",
      "Requirement already satisfied: h11<1,>=0.9.0 in /opt/conda/lib/python3.11/site-packages (from wsproto>=0.14->trio-websocket~=0.9->selenium) (0.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas selenium webdriver-manager pyarrow s3fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e9e6faf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-18 17:40:03,672 - INFO - 3296181972.py:204 - Configured LakeFS Target Path: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:40:03,673 - INFO - 3296181972.py:205 - Scraping interval set to 5 minutes.\n",
      "2025-05-18 17:40:03,673 - INFO - 3296181972.py:27 - Initializing WebDriver...\n",
      "2025-05-18 17:40:03,674 - INFO - logger.py:11 - ====== WebDriver manager ======\n",
      "2025-05-18 17:40:03,784 - INFO - logger.py:11 - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-18 17:40:03,887 - INFO - logger.py:11 - Get LATEST chromedriver version for google-chrome\n",
      "2025-05-18 17:40:03,976 - INFO - logger.py:11 - Driver [/home/jovyan/.wdm/drivers/chromedriver/linux64/136.0.7103.94/chromedriver-linux64/chromedriver] found in cache\n",
      "2025-05-18 17:40:04,353 - INFO - 3296181972.py:40 - WebDriver initialized successfully.\n",
      "2025-05-18 17:40:04,355 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 17:40:04,357 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:40:04,956 - INFO - 3296181972.py:119 - Successfully read 1 rows from existing Parquet file.\n",
      "2025-05-18 17:40:04,958 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 17:40:05,713 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 17:40:15,720 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 17:40:15', 'display_date_id': '240', 'display_time': '00:04', 'current_value_MW': 25061.4, 'temperature_C': 29.6}\n",
      "2025-05-18 17:40:15,723 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 17:40:15', 'display_date_id': '240', 'display_time': '00:04', 'current_value_MW': 25061.4, 'temperature_C': 29.6}\n",
      "2025-05-18 17:40:15,733 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 17:40:15,738 - INFO - 3296181972.py:150 - Rows after deduplication: 2\n",
      "2025-05-18 17:40:15,747 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 17:40:15,748 - INFO - 3296181972.py:175 - Saving 2 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:40:15,780 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 17:40:15,782 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 17:40:15,783 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 17:45:15,757 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 17:45:15,759 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:45:15,830 - INFO - 3296181972.py:119 - Successfully read 2 rows from existing Parquet file.\n",
      "2025-05-18 17:45:15,831 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 17:45:15,939 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 17:45:25,942 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 17:45:25', 'display_date_id': '540', 'display_time': '00:09', 'current_value_MW': 24910.0, 'temperature_C': 29.6}\n",
      "2025-05-18 17:45:25,942 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 17:45:25', 'display_date_id': '540', 'display_time': '00:09', 'current_value_MW': 24910.0, 'temperature_C': 29.6}\n",
      "2025-05-18 17:45:25,946 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 17:45:25,950 - INFO - 3296181972.py:150 - Rows after deduplication: 3\n",
      "2025-05-18 17:45:25,958 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 17:45:25,959 - INFO - 3296181972.py:175 - Saving 3 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:45:25,985 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 17:45:25,986 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 17:45:25,987 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 17:50:25,971 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 17:50:25,974 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:50:26,037 - INFO - 3296181972.py:119 - Successfully read 3 rows from existing Parquet file.\n",
      "2025-05-18 17:50:26,038 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 17:50:26,631 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 17:50:36,637 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 17:50:36', 'display_date_id': '960', 'display_time': '00:16', 'current_value_MW': 24715.7, 'temperature_C': 29.6}\n",
      "2025-05-18 17:50:36,638 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 17:50:36', 'display_date_id': '960', 'display_time': '00:16', 'current_value_MW': 24715.7, 'temperature_C': 29.6}\n",
      "2025-05-18 17:50:36,641 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 17:50:36,643 - INFO - 3296181972.py:150 - Rows after deduplication: 4\n",
      "2025-05-18 17:50:36,647 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 17:50:36,648 - INFO - 3296181972.py:175 - Saving 4 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:50:36,671 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 17:50:36,672 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 17:50:36,673 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 17:55:36,656 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 17:55:36,662 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:55:36,731 - INFO - 3296181972.py:119 - Successfully read 4 rows from existing Parquet file.\n",
      "2025-05-18 17:55:36,733 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 17:55:36,801 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 17:55:46,808 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 17:55:46', 'display_date_id': '1200', 'display_time': '00:20', 'current_value_MW': 24662.9, 'temperature_C': 29.6}\n",
      "2025-05-18 17:55:46,809 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 17:55:46', 'display_date_id': '1200', 'display_time': '00:20', 'current_value_MW': 24662.9, 'temperature_C': 29.6}\n",
      "2025-05-18 17:55:46,815 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 17:55:46,822 - INFO - 3296181972.py:150 - Rows after deduplication: 5\n",
      "2025-05-18 17:55:46,829 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 17:55:46,830 - INFO - 3296181972.py:175 - Saving 5 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 17:55:46,857 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 17:55:46,859 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 17:55:46,860 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:00:46,844 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:00:46,848 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:00:46,909 - INFO - 3296181972.py:119 - Successfully read 5 rows from existing Parquet file.\n",
      "2025-05-18 18:00:46,911 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:00:47,016 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:00:57,019 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:00:57', 'display_date_id': '1800', 'display_time': '00:30', 'current_value_MW': 24428.3, 'temperature_C': 29.6}\n",
      "2025-05-18 18:00:57,020 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:00:57', 'display_date_id': '1800', 'display_time': '00:30', 'current_value_MW': 24428.3, 'temperature_C': 29.6}\n",
      "2025-05-18 18:00:57,023 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:00:57,026 - INFO - 3296181972.py:150 - Rows after deduplication: 6\n",
      "2025-05-18 18:00:57,031 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:00:57,032 - INFO - 3296181972.py:175 - Saving 6 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:00:57,055 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:00:57,055 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:00:57,056 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:05:57,039 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:05:57,043 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:05:57,129 - INFO - 3296181972.py:119 - Successfully read 6 rows from existing Parquet file.\n",
      "2025-05-18 18:05:57,130 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:05:57,194 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:06:07,198 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:06:07', 'display_date_id': '2940', 'display_time': '00:49', 'current_value_MW': 23971.1, 'temperature_C': 29.5}\n",
      "2025-05-18 18:06:07,199 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:06:07', 'display_date_id': '2940', 'display_time': '00:49', 'current_value_MW': 23971.1, 'temperature_C': 29.5}\n",
      "2025-05-18 18:06:07,201 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:06:07,204 - INFO - 3296181972.py:150 - Rows after deduplication: 7\n",
      "2025-05-18 18:06:07,208 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:06:07,209 - INFO - 3296181972.py:175 - Saving 7 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:06:07,231 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:06:07,232 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:06:07,233 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:11:07,215 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:11:07,220 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:11:07,305 - INFO - 3296181972.py:119 - Successfully read 7 rows from existing Parquet file.\n",
      "2025-05-18 18:11:07,307 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:11:07,395 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:11:17,403 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:11:17', 'display_date_id': '3600', 'display_time': '01:00', 'current_value_MW': 23816.7, 'temperature_C': 29.4}\n",
      "2025-05-18 18:11:17,404 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:11:17', 'display_date_id': '3600', 'display_time': '01:00', 'current_value_MW': 23816.7, 'temperature_C': 29.4}\n",
      "2025-05-18 18:11:17,412 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:11:17,418 - INFO - 3296181972.py:150 - Rows after deduplication: 8\n",
      "2025-05-18 18:11:17,428 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:11:17,430 - INFO - 3296181972.py:175 - Saving 8 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:11:17,460 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:11:17,462 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:11:17,464 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:16:17,448 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:16:17,460 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:16:17,568 - INFO - 3296181972.py:119 - Successfully read 8 rows from existing Parquet file.\n",
      "2025-05-18 18:16:17,571 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:16:17,717 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:16:27,720 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:16:27', 'display_date_id': '4440', 'display_time': '01:14', 'current_value_MW': 23516.9, 'temperature_C': 29.4}\n",
      "2025-05-18 18:16:27,721 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:16:27', 'display_date_id': '4440', 'display_time': '01:14', 'current_value_MW': 23516.9, 'temperature_C': 29.4}\n",
      "2025-05-18 18:16:27,725 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:16:27,729 - INFO - 3296181972.py:150 - Rows after deduplication: 9\n",
      "2025-05-18 18:16:27,736 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:16:27,737 - INFO - 3296181972.py:175 - Saving 9 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:16:27,762 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:16:27,764 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:16:27,765 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:21:27,748 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:21:27,752 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:21:27,831 - INFO - 3296181972.py:119 - Successfully read 9 rows from existing Parquet file.\n",
      "2025-05-18 18:21:27,833 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:21:27,902 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:21:37,907 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:21:37', 'display_date_id': '4740', 'display_time': '01:19', 'current_value_MW': 23630.8, 'temperature_C': 29.4}\n",
      "2025-05-18 18:21:37,907 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:21:37', 'display_date_id': '4740', 'display_time': '01:19', 'current_value_MW': 23630.8, 'temperature_C': 29.4}\n",
      "2025-05-18 18:21:37,912 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:21:37,915 - INFO - 3296181972.py:150 - Rows after deduplication: 10\n",
      "2025-05-18 18:21:37,919 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:21:37,920 - INFO - 3296181972.py:175 - Saving 10 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:21:37,943 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:21:37,944 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:21:37,945 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:26:37,928 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:26:37,936 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:26:38,022 - INFO - 3296181972.py:119 - Successfully read 10 rows from existing Parquet file.\n",
      "2025-05-18 18:26:38,024 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:26:38,096 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:26:48,101 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:26:48', 'display_date_id': '5100', 'display_time': '01:25', 'current_value_MW': 23335.9, 'temperature_C': 29.4}\n",
      "2025-05-18 18:26:48,102 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:26:48', 'display_date_id': '5100', 'display_time': '01:25', 'current_value_MW': 23335.9, 'temperature_C': 29.4}\n",
      "2025-05-18 18:26:48,106 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:26:48,109 - INFO - 3296181972.py:150 - Rows after deduplication: 11\n",
      "2025-05-18 18:26:48,114 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:26:48,115 - INFO - 3296181972.py:175 - Saving 11 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:26:48,141 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:26:48,141 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:26:48,142 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:31:48,112 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:31:48,116 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:31:48,211 - INFO - 3296181972.py:119 - Successfully read 11 rows from existing Parquet file.\n",
      "2025-05-18 18:31:48,215 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:31:48,348 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:31:58,349 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:31:58', 'display_date_id': '5340', 'display_time': '01:29', 'current_value_MW': 23445.5, 'temperature_C': 29.4}\n",
      "2025-05-18 18:31:58,350 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:31:58', 'display_date_id': '5340', 'display_time': '01:29', 'current_value_MW': 23445.5, 'temperature_C': 29.4}\n",
      "2025-05-18 18:31:58,352 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:31:58,356 - INFO - 3296181972.py:150 - Rows after deduplication: 12\n",
      "2025-05-18 18:31:58,359 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:31:58,360 - INFO - 3296181972.py:175 - Saving 12 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:31:58,386 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:31:58,387 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:31:58,388 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:36:58,352 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:36:58,355 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:36:58,434 - INFO - 3296181972.py:119 - Successfully read 12 rows from existing Parquet file.\n",
      "2025-05-18 18:36:58,437 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:36:58,508 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:37:08,513 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:37:08', 'display_date_id': '5700', 'display_time': '01:35', 'current_value_MW': 23316.9, 'temperature_C': 29.4}\n",
      "2025-05-18 18:37:08,514 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:37:08', 'display_date_id': '5700', 'display_time': '01:35', 'current_value_MW': 23316.9, 'temperature_C': 29.4}\n",
      "2025-05-18 18:37:08,517 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:37:08,520 - INFO - 3296181972.py:150 - Rows after deduplication: 13\n",
      "2025-05-18 18:37:08,524 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:37:08,525 - INFO - 3296181972.py:175 - Saving 13 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:37:08,552 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:37:08,553 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:37:08,554 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:42:08,514 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:42:08,519 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:42:08,592 - INFO - 3296181972.py:119 - Successfully read 13 rows from existing Parquet file.\n",
      "2025-05-18 18:42:08,594 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:42:08,718 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:42:18,726 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:42:18', 'display_date_id': '6060', 'display_time': '01:41', 'current_value_MW': 23101.8, 'temperature_C': 29.4}\n",
      "2025-05-18 18:42:18,727 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:42:18', 'display_date_id': '6060', 'display_time': '01:41', 'current_value_MW': 23101.8, 'temperature_C': 29.4}\n",
      "2025-05-18 18:42:18,732 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:42:18,735 - INFO - 3296181972.py:150 - Rows after deduplication: 14\n",
      "2025-05-18 18:42:18,748 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:42:18,749 - INFO - 3296181972.py:175 - Saving 14 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:42:18,780 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:42:18,782 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:42:18,783 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:47:18,746 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:47:18,751 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:47:18,838 - INFO - 3296181972.py:119 - Successfully read 14 rows from existing Parquet file.\n",
      "2025-05-18 18:47:18,840 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:47:18,966 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:47:28,969 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:47:28', 'display_date_id': '6300', 'display_time': '01:45', 'current_value_MW': 23008.2, 'temperature_C': 29.4}\n",
      "2025-05-18 18:47:28,971 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:47:28', 'display_date_id': '6300', 'display_time': '01:45', 'current_value_MW': 23008.2, 'temperature_C': 29.4}\n",
      "2025-05-18 18:47:28,978 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:47:28,986 - INFO - 3296181972.py:150 - Rows after deduplication: 15\n",
      "2025-05-18 18:47:28,995 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:47:28,997 - INFO - 3296181972.py:175 - Saving 15 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:47:29,031 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:47:29,032 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:47:29,034 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:52:29,005 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:52:29,007 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:52:29,103 - INFO - 3296181972.py:119 - Successfully read 15 rows from existing Parquet file.\n",
      "2025-05-18 18:52:29,106 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:52:29,236 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:52:39,243 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:52:39', 'display_date_id': '6660', 'display_time': '01:51', 'current_value_MW': 22980.8, 'temperature_C': 29.4}\n",
      "2025-05-18 18:52:39,244 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:52:39', 'display_date_id': '6660', 'display_time': '01:51', 'current_value_MW': 22980.8, 'temperature_C': 29.4}\n",
      "2025-05-18 18:52:39,248 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:52:39,250 - INFO - 3296181972.py:150 - Rows after deduplication: 16\n",
      "2025-05-18 18:52:39,254 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:52:39,256 - INFO - 3296181972.py:175 - Saving 16 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:52:39,277 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:52:39,278 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:52:39,279 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:57:39,254 - INFO - 3296181972.py:113 - --- Starting new scrape and update cycle ---\n",
      "2025-05-18 18:57:39,256 - INFO - 3296181972.py:117 - Attempting to read existing Parquet from: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:57:39,326 - INFO - 3296181972.py:119 - Successfully read 16 rows from existing Parquet file.\n",
      "2025-05-18 18:57:39,328 - INFO - 3296181972.py:89 - Navigating to URL: https://www.sothailand.com/sysgen/egat/\n",
      "2025-05-18 18:57:39,425 - INFO - 3296181972.py:91 - Waiting for page load and data (10 seconds)...\n",
      "2025-05-18 18:57:49,429 - INFO - 3296181972.py:74 - Data extracted: {'scrape_timestamp_utc': '2025-05-18 18:57:49', 'display_date_id': '6900', 'display_time': '01:55', 'current_value_MW': 22974.5, 'temperature_C': 29.4}\n",
      "2025-05-18 18:57:49,430 - INFO - 3296181972.py:139 - Scraped new data: {'scrape_timestamp_utc': '2025-05-18 18:57:49', 'display_date_id': '6900', 'display_time': '01:55', 'current_value_MW': 22974.5, 'temperature_C': 29.4}\n",
      "2025-05-18 18:57:49,433 - INFO - 3296181972.py:147 - Deduplicating data...\n",
      "2025-05-18 18:57:49,435 - INFO - 3296181972.py:150 - Rows after deduplication: 17\n",
      "2025-05-18 18:57:49,441 - INFO - 3296181972.py:168 - Sorted final DataFrame.\n",
      "2025-05-18 18:57:49,442 - INFO - 3296181972.py:175 - Saving 17 rows to LakeFS: s3a://dataset/main/egat_datascraping/egat_realtime_power_history.parquet\n",
      "2025-05-18 18:57:49,464 - INFO - 3296181972.py:181 - Successfully saved data to LakeFS.\n",
      "2025-05-18 18:57:49,464 - INFO - 3296181972.py:186 - --- Scrape and update cycle finished ---\n",
      "2025-05-18 18:57:49,465 - INFO - 3296181972.py:218 - Waiting for 5 minutes (300 seconds) before next cycle...\n",
      "2025-05-18 18:59:23,993 - INFO - 3296181972.py:222 - Process interrupted by user (KeyboardInterrupt).\n",
      "2025-05-18 18:59:23,995 - INFO - 3296181972.py:100 - Closing WebDriver.\n",
      "2025-05-18 18:59:24,002 - WARNING - connectionpool.py:872 - Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa70c18f150>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c6f30d5e899404661c8a6fa9ae23bc3e\n",
      "2025-05-18 18:59:24,006 - WARNING - connectionpool.py:872 - Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa705cd7990>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c6f30d5e899404661c8a6fa9ae23bc3e\n",
      "2025-05-18 18:59:24,009 - WARNING - connectionpool.py:872 - Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'NewConnectionError('<urllib3.connection.HTTPConnection object at 0x7fa705cee1d0>: Failed to establish a new connection: [Errno 111] Connection refused')': /session/c6f30d5e899404661c8a6fa9ae23bc3e\n",
      "2025-05-18 18:59:24,015 - INFO - 3296181972.py:228 - Scraping process terminated.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "import os\n",
    "import re\n",
    "import logging\n",
    "\n",
    "# --- Logging Configuration ---\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',\n",
    "    handlers=[logging.StreamHandler()]\n",
    ")\n",
    "\n",
    "# --- EGATRealTimeScraper Class ---\n",
    "class EGATRealTimeScraper:\n",
    "    def __init__(self, url=\"https://www.sothailand.com/sysgen/egat/\"):\n",
    "        self.url = url\n",
    "        self.driver = None\n",
    "        self._initialize_driver()\n",
    "\n",
    "    def _initialize_driver(self):\n",
    "        logging.info(\"Initializing WebDriver...\")\n",
    "        chrome_options = Options()\n",
    "        chrome_options.add_argument(\"--headless\")\n",
    "        chrome_options.add_argument(\"--no-sandbox\")\n",
    "        chrome_options.add_argument(\"--disable-dev-shm-usage\")\n",
    "        chrome_options.add_argument(\"--disable-gpu\")\n",
    "        chrome_options.add_argument(\"--window-size=1920,1080\")\n",
    "        chrome_options.add_argument(\"--log-level=0\")\n",
    "        chrome_options.set_capability('goog:loggingPrefs', {'browser': 'ALL'})\n",
    "\n",
    "        try:\n",
    "            service = Service(ChromeDriverManager().install())\n",
    "            self.driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "            logging.info(\"WebDriver initialized successfully.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error initializing WebDriver: {e}\", exc_info=True)\n",
    "            self.driver = None\n",
    "\n",
    "    def extract_data_from_console(self):\n",
    "        if not self.driver:\n",
    "            logging.error(\"WebDriver not initialized. Cannot extract data.\")\n",
    "            return None\n",
    "        try:\n",
    "            logs = self.driver.get_log('browser')\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to get browser logs: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "        for log_entry in reversed(logs):\n",
    "            message = log_entry.get('message', '')\n",
    "            if 'updateMessageArea:' in message:\n",
    "                match = re.search(r'updateMessageArea:\\s*(\\d+)\\s*,\\s*(\\d{1,2}:\\d{2})\\s*,\\s*([\\d,]+\\.?\\d*)\\s*,\\s*(\\d+\\.?\\d*)', message)\n",
    "                if match:\n",
    "                    display_date_id = match.group(1).strip()\n",
    "                    display_time = match.group(2).strip()\n",
    "                    current_value_mw_str = match.group(3).replace(',', '').strip()\n",
    "                    temperature_c_str = match.group(4).strip()\n",
    "                    try:\n",
    "                        current_value_mw = float(current_value_mw_str) if current_value_mw_str else None\n",
    "                        temperature_c = float(temperature_c_str) if temperature_c_str else None\n",
    "                        data_dict = {\n",
    "                            'scrape_timestamp_utc': datetime.datetime.utcnow().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "                            'display_date_id': display_date_id,\n",
    "                            'display_time': display_time,\n",
    "                            'current_value_MW': current_value_mw,\n",
    "                            'temperature_C': temperature_c\n",
    "                        }\n",
    "                        logging.info(f\"Data extracted: {data_dict}\")\n",
    "                        return data_dict\n",
    "                    except ValueError as ve:\n",
    "                        logging.error(f\"Error converting extracted data: {ve}. Raw: val='{current_value_mw_str}', temp='{temperature_c_str}'\")\n",
    "        logging.warning(\"Relevant 'updateMessageArea' log not found or data parsing failed.\")\n",
    "        return None\n",
    "\n",
    "    def scrape_once(self):\n",
    "        if not self.driver:\n",
    "            logging.warning(\"WebDriver not available. Attempting to re-initialize.\")\n",
    "            self._initialize_driver()\n",
    "            if not self.driver:\n",
    "                 logging.error(\"Failed to re-initialize WebDriver. Aborting scrape_once.\")\n",
    "                 return None\n",
    "        try:\n",
    "            logging.info(f\"Navigating to URL: {self.url}\")\n",
    "            self.driver.get(self.url)\n",
    "            logging.info(\"Waiting for page load and data (10 seconds)...\")\n",
    "            time.sleep(10)\n",
    "            return self.extract_data_from_console()\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Error during scrape_once: {e}\", exc_info=True)\n",
    "            return None\n",
    "\n",
    "    def close(self):\n",
    "        if self.driver:\n",
    "            logging.info(\"Closing WebDriver.\")\n",
    "            try:\n",
    "                self.driver.quit()\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error quitting WebDriver: {e}\", exc_info=True)\n",
    "            finally:\n",
    "                self.driver = None\n",
    "\n",
    "# --- Function for a single scrape and update cycle (with robust try-except for read_parquet) ---\n",
    "def perform_scrape_and_update(scraper, lakefs_s3_path, storage_options):\n",
    "    \"\"\"\n",
    "    Performs one cycle of scraping, processing, and updating data to LakeFS.\n",
    "    \"\"\"\n",
    "    logging.info(\"--- Starting new scrape and update cycle ---\")\n",
    "    existing_df = pd.DataFrame() # Initialize as empty DataFrame\n",
    "\n",
    "    try:\n",
    "        logging.info(f\"Attempting to read existing Parquet from: {lakefs_s3_path}\")\n",
    "        existing_df = pd.read_parquet(lakefs_s3_path, storage_options=storage_options)\n",
    "        logging.info(f\"Successfully read {len(existing_df)} rows from existing Parquet file.\")\n",
    "        if 'scrape_timestamp_utc' in existing_df.columns:\n",
    "            existing_df['scrape_timestamp_utc'] = pd.to_datetime(existing_df['scrape_timestamp_utc'], errors='coerce')\n",
    "    except FileNotFoundError:\n",
    "        logging.warning(f\"Parquet file not found at {lakefs_s3_path} (FileNotFoundError). This is expected if it's the first run. A new file will be created.\")\n",
    "    except Exception as e:\n",
    "        error_message = str(e).lower()\n",
    "        # Check for common \"not found\" messages from S3/PyArrow\n",
    "        if \"no such file or directory\" in error_message or \\\n",
    "           \"nosuchkey\" in error_message or \\\n",
    "           \"nosuchbucket\" in error_message or \\\n",
    "           isinstance(e, pd.errors.EmptyDataError): # EmptyDataError can occur if file is empty/unreadable\n",
    "            logging.warning(f\"Parquet file not found or unreadable at {lakefs_s3_path} (Error: {type(e).__name__} - {e}). This is expected if it's the first run or an empty/corrupt file. A new/updated file will be created.\")\n",
    "        else:\n",
    "            logging.error(f\"Unexpected error reading Parquet file from {lakefs_s3_path}: {type(e).__name__} - {e}\", exc_info=True)\n",
    "            # For now, allow script to continue with an empty existing_df\n",
    "\n",
    "    new_data_dict = scraper.scrape_once()\n",
    "\n",
    "    if new_data_dict:\n",
    "        logging.info(f\"Scraped new data: {new_data_dict}\")\n",
    "        new_df = pd.DataFrame([new_data_dict])\n",
    "        new_df['scrape_timestamp_utc'] = pd.to_datetime(new_df['scrape_timestamp_utc'], errors='coerce')\n",
    "\n",
    "        combined_df = pd.concat([existing_df, new_df], ignore_index=True) if not existing_df.empty else new_df\n",
    "\n",
    "        key_cols_for_dedup = ['display_date_id', 'display_time']\n",
    "        if all(col in combined_df.columns for col in key_cols_for_dedup) and 'scrape_timestamp_utc' in combined_df.columns:\n",
    "            logging.info(\"Deduplicating data...\")\n",
    "            combined_df.sort_values('scrape_timestamp_utc', ascending=False, inplace=True, na_position='last')\n",
    "            deduplicated_df = combined_df.drop_duplicates(subset=key_cols_for_dedup, keep='first')\n",
    "            logging.info(f\"Rows after deduplication: {len(deduplicated_df)}\")\n",
    "        else:\n",
    "            deduplicated_df = combined_df\n",
    "            logging.warning(\"Skipping deduplication due to missing key columns.\")\n",
    "\n",
    "        try:\n",
    "            if 'display_date_id' in deduplicated_df.columns and 'display_time' in deduplicated_df.columns:\n",
    "                deduplicated_df['temp_datetime_sort'] = pd.to_datetime(\n",
    "                    deduplicated_df['display_date_id'] + ' ' + deduplicated_df['display_time'],\n",
    "                    format='%Y%m%d %H:%M', errors='coerce'\n",
    "                )\n",
    "                deduplicated_df.sort_values(\n",
    "                    by=['temp_datetime_sort', 'scrape_timestamp_utc'],\n",
    "                    ascending=[True, True], inplace=True, na_position='last'\n",
    "                )\n",
    "                deduplicated_df.drop(columns=['temp_datetime_sort'], inplace=True)\n",
    "            elif 'scrape_timestamp_utc' in deduplicated_df.columns:\n",
    "                 deduplicated_df.sort_values('scrape_timestamp_utc', ascending=True, inplace=True, na_position='last')\n",
    "            logging.info(\"Sorted final DataFrame.\")\n",
    "        except Exception as sort_e:\n",
    "            logging.warning(f\"Could not perform full sort: {sort_e}. Using basic sort if possible.\")\n",
    "            if 'scrape_timestamp_utc' in deduplicated_df.columns: # Fallback sort\n",
    "                 deduplicated_df.sort_values('scrape_timestamp_utc', ascending=True, inplace=True, na_position='last')\n",
    "\n",
    "        try:\n",
    "            logging.info(f\"Saving {len(deduplicated_df)} rows to LakeFS: {lakefs_s3_path}\")\n",
    "            deduplicated_df.to_parquet(\n",
    "                lakefs_s3_path,\n",
    "                storage_options=storage_options,\n",
    "                index=False, engine='pyarrow', compression='snappy'\n",
    "            )\n",
    "            logging.info(\"Successfully saved data to LakeFS.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Failed to save DataFrame to LakeFS: {e}\", exc_info=True)\n",
    "    else:\n",
    "        logging.warning(\"No new data was scraped in this cycle.\")\n",
    "    logging.info(\"--- Scrape and update cycle finished ---\")\n",
    "\n",
    "def run_scraper_periodically(interval_minutes=5):\n",
    "    ACCESS_KEY = os.getenv(\"LAKEFS_ACCESS_KEY_ID\", \"access_key\")\n",
    "    SECRET_KEY = os.getenv(\"LAKEFS_SECRET_ACCESS_KEY\", \"secret_key\")\n",
    "    LAKEFS_ENDPOINT = os.getenv(\"LAKEFS_ENDPOINT_URL\", \"http://lakefs-dev:8000/\")\n",
    "    REPO_NAME = \"dataset\"\n",
    "    BRANCH_NAME = \"main\"\n",
    "    TARGET_PARQUET_FILE_PATH = \"egat_datascraping/egat_realtime_power_history.parquet\"\n",
    "    lakefs_s3_path = f\"s3a://{REPO_NAME}/{BRANCH_NAME}/{TARGET_PARQUET_FILE_PATH}\"\n",
    "\n",
    "    storage_options = {\n",
    "        \"key\": ACCESS_KEY,\n",
    "        \"secret\": SECRET_KEY,\n",
    "        \"client_kwargs\": {\n",
    "            \"endpoint_url\": LAKEFS_ENDPOINT\n",
    "        }\n",
    "    }\n",
    "    logging.info(f\"Configured LakeFS Target Path: {lakefs_s3_path}\")\n",
    "    logging.info(f\"Scraping interval set to {interval_minutes} minutes.\")\n",
    "\n",
    "    scraper = None\n",
    "    try:\n",
    "        scraper = EGATRealTimeScraper()\n",
    "        if not scraper.driver:\n",
    "            logging.error(\"WebDriver could not be initialized. Terminating process.\")\n",
    "            return\n",
    "\n",
    "        while True:\n",
    "            perform_scrape_and_update(scraper, lakefs_s3_path, storage_options)\n",
    "            \n",
    "            wait_seconds = interval_minutes * 60\n",
    "            logging.info(f\"Waiting for {interval_minutes} minutes ({wait_seconds} seconds) before next cycle...\")\n",
    "            time.sleep(wait_seconds)\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        logging.info(\"Process interrupted by user (KeyboardInterrupt).\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"An unexpected error occurred in the main loop: {e}\", exc_info=True)\n",
    "    finally:\n",
    "        if scraper:\n",
    "            scraper.close()\n",
    "        logging.info(\"Scraping process terminated.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_scraper_periodically(interval_minutes=5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
